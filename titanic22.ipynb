{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Logging into Kaggle for the first time can be daunting.\nOur competitions often have large cash prizes, public leaderboards, and involve complex data. Nevertheless, we really think all data scientists can rapidly learn from machine learning competitions and meaningfully contribute to our community. To give you a clear understanding of how our platform works and a mental model of the type of learning you could do on Kaggle, we've created a Getting Started tutorial for the Titanic competition. It walks you through the initial steps required to get your first decent submission on the leaderboard. By the end of the tutorial, you'll also have a solid understanding of how to use Kaggle's online coding environment, where you'll have trained your own machine learning model.","metadata":{}},{"cell_type":"markdown","source":"![](https://www.dataquest.io/wp-content/uploads/2017/12/kaggle-fundamentals.jpg)","metadata":{}},{"cell_type":"markdown","source":"## Install the required libraries:\n  - `pandas` - needed to work with data and represent them in pd.DataFrame for more convenient interaction\n  - `numpy` - a library needed for highly efficient work with arrays and with mathematical functions\n- `os` - a library for selecting folders where data is downloaded\n  - `glob` - to select files for parsing, helps to select files with a certain extension","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nos.chdir('/kaggle/input/c/titanic/')\n\nall_file_names = [i for i in glob.glob('*.csv')]\nall_file_names","metadata":{"execution":{"iopub.status.busy":"2021-12-12T12:52:16.310034Z","iopub.execute_input":"2021-12-12T12:52:16.3108Z","iopub.status.idle":"2021-12-12T12:52:16.320499Z","shell.execute_reply.started":"2021-12-12T12:52:16.310759Z","shell.execute_reply":"2021-12-12T12:52:16.319691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# change the dataframe display setting so that all columns are displayed\npd.options.display.max_columns = None","metadata":{"execution":{"iopub.status.busy":"2021-12-12T12:57:10.118512Z","iopub.execute_input":"2021-12-12T12:57:10.118809Z","iopub.status.idle":"2021-12-12T12:57:10.123262Z","shell.execute_reply.started":"2021-12-12T12:57:10.11878Z","shell.execute_reply":"2021-12-12T12:57:10.122564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/c/titanic/train.csv')\ndf","metadata":{"execution":{"iopub.status.busy":"2021-12-12T12:57:10.521771Z","iopub.execute_input":"2021-12-12T12:57:10.522202Z","iopub.status.idle":"2021-12-12T12:57:10.570579Z","shell.execute_reply.started":"2021-12-12T12:57:10.522168Z","shell.execute_reply":"2021-12-12T12:57:10.569832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/c/titanic/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-12T12:58:01.778751Z","iopub.execute_input":"2021-12-12T12:58:01.779502Z","iopub.status.idle":"2021-12-12T12:58:01.788371Z","shell.execute_reply.started":"2021-12-12T12:58:01.779466Z","shell.execute_reply":"2021-12-12T12:58:01.787753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Where are not using first column, cause its dubble for id column ","metadata":{}},{"cell_type":"code","source":"df.drop(df.columns[0], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T20:07:36.485527Z","iopub.execute_input":"2021-12-09T20:07:36.486422Z","iopub.status.idle":"2021-12-09T20:07:36.497377Z","shell.execute_reply.started":"2021-12-09T20:07:36.486375Z","shell.execute_reply":"2021-12-09T20:07:36.496123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Name'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T20:14:55.078205Z","iopub.execute_input":"2021-12-09T20:14:55.078975Z","iopub.status.idle":"2021-12-09T20:14:55.088552Z","shell.execute_reply.started":"2021-12-09T20:14:55.078934Z","shell.execute_reply":"2021-12-09T20:14:55.087747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have prefixes in the name, the type of Mrs. there, the captain, etc., let's see if this affects the model","metadata":{}},{"cell_type":"code","source":"df = pd.concat([df.drop('Name', axis=1), df_dum])","metadata":{"execution":{"iopub.status.busy":"2021-12-09T20:21:23.941883Z","iopub.execute_input":"2021-12-09T20:21:23.942188Z","iopub.status.idle":"2021-12-09T20:21:23.951461Z","shell.execute_reply.started":"2021-12-09T20:21:23.942157Z","shell.execute_reply":"2021-12-09T20:21:23.950575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create words that will group the signs\n# The second word reflects the semantic load\nsearchMr = ['Mr' ]\nsearchDr = ['Dr' ]\nsearchMrs = ['Mrs']\nsearchMiss = ['Miss']\nsearchMadam = ['Madam']\n\n\n\ndef search_col(df_dum, search):\n\"\"\"function for finding columns in which there were matches by trigger words\n     the input is a list with triggers and a Dataframe, in which you need to find the columns \"\"\"\n    matches=[]\n    for col in df_dum.columns:\n        for match in search:\n            if match in col:\n                matches.append(col)\n    return matches\n\n\ndef boolen(num):\n\"\"\"The function for converting numbers to the bool type on the input, numbers on the output True or False\n     Depending on 'the number is not 0' \"\"\"\n    if num==0:\n        return False\n    return True\n\n\n\ndef conc(Name ,df, matches):\n    \"\"\"A function to group columns and convert data to bool using the boolen function.\n     the input of the function is the desired name of the grouped columns (Name), DataFrame with the required columns\n      and the columns needed for grouping. The output is a DataFrame without columns that needed to be grouped, but\n      with a new column - grouping\"\"\"\n    df[Name] = False\n    for match in matches:\n        df[Name] = df[Name]+df[match]\n    df[Name] = df[Name].apply(boolen)\n    return df.drop(matches, axis=1)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-09T20:19:15.534078Z","iopub.execute_input":"2021-12-09T20:19:15.534867Z","iopub.status.idle":"2021-12-09T20:19:15.543016Z","shell.execute_reply.started":"2021-12-09T20:19:15.534825Z","shell.execute_reply":"2021-12-09T20:19:15.542418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"searchWinter = ['cyt;', \"ktl\"]\nSearchVision = ['dbl','hfpk']\nSearchSign = ['ghbv','ncen']\n\nwint = search_col(df, searchWinter)\n\nsign = search_col(df, SearchSign)\n\nvis = search_col(df, SearchVision)\n\ndf = conc('BadWinter', df, wint)\ndf = conc('BadSign', df, sign)\ndf = conc('BadVision', df, vis)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = conc('Miss', df, search_col(df, searchMiss))\ndf = conc('Capt', df, search_col(df, ['Capt']))\ndf = conc('Col', df, search_col(df, ['Col']))\ndf = conc('Dr', df, search_col(df, ['Dr']))\ndf = conc('Master', df, search_col(df, ['Master']))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T20:24:45.600116Z","iopub.execute_input":"2021-12-09T20:24:45.600399Z","iopub.status.idle":"2021-12-09T20:24:45.643644Z","shell.execute_reply.started":"2021-12-09T20:24:45.60037Z","shell.execute_reply":"2021-12-09T20:24:45.642709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This method can be used to group data, but not if there are a lot of columns.","metadata":{}},{"cell_type":"markdown","source":"Since we have a lot of signs, and many of them are used extremely rarely, we need to get rid of them\nWe will get rid of features that are obtained by binarization of text features\nSince these values have a bad effect on the generalizing ability of the future model","metadata":{}},{"cell_type":"code","source":"df.drop(['Cabin'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T20:31:05.10482Z","iopub.execute_input":"2021-12-09T20:31:05.105142Z","iopub.status.idle":"2021-12-09T20:31:05.11203Z","shell.execute_reply.started":"2021-12-09T20:31:05.105107Z","shell.execute_reply":"2021-12-09T20:31:05.111002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T20:31:18.864801Z","iopub.execute_input":"2021-12-09T20:31:18.865118Z","iopub.status.idle":"2021-12-09T20:31:18.873827Z","shell.execute_reply.started":"2021-12-09T20:31:18.865081Z","shell.execute_reply":"2021-12-09T20:31:18.872963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T20:31:25.15606Z","iopub.execute_input":"2021-12-09T20:31:25.156333Z","iopub.status.idle":"2021-12-09T20:31:25.171769Z","shell.execute_reply.started":"2021-12-09T20:31:25.156304Z","shell.execute_reply":"2021-12-09T20:31:25.170776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing the impact of attributes\nOur visualization will consist of:\n- correlation matrix\n- Histograms of the influence of signs\n- Boxes with mustaches","metadata":{}},{"cell_type":"code","source":"# seaborn library for correlation matrix\nimport seaborn as sns\n#ploty for boxplots and histograms of the influence of features\nimport plotly.express as px\n#pyplot to increase the size of the plot\nimport matplotlib.pyplot as plt\n\nfrom plotly.offline import init_notebook_mode, iplot\n\ninit_notebook_mode(connected=True)  ","metadata":{"execution":{"iopub.status.busy":"2021-12-12T13:04:54.897774Z","iopub.execute_input":"2021-12-12T13:04:54.898574Z","iopub.status.idle":"2021-12-12T13:04:57.035776Z","shell.execute_reply.started":"2021-12-12T13:04:54.898539Z","shell.execute_reply":"2021-12-12T13:04:57.034916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check each binary column for usage> 10%\nproc = len(df)*0.1\nfor col in df.columns[11:]:\n    if df[col].sum()<proc:\n        df.drop(col, inplace=True, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T20:28:57.907439Z","iopub.execute_input":"2021-12-09T20:28:57.907724Z","iopub.status.idle":"2021-12-09T20:28:57.946934Z","shell.execute_reply.started":"2021-12-09T20:28:57.907691Z","shell.execute_reply":"2021-12-09T20:28:57.946072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# increase the size\nplt.figure(figsize=(20,20))\n\n# build the correlation matrix\nsns.heatmap(df.corr() ,annot=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-12T13:04:58.519961Z","iopub.execute_input":"2021-12-12T13:04:58.520221Z","iopub.status.idle":"2021-12-12T13:04:59.074756Z","shell.execute_reply.started":"2021-12-12T13:04:58.520194Z","shell.execute_reply":"2021-12-12T13:04:59.073953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 12))\nheatmap = sns.heatmap(df.corr()[['Survived']].sort_values(by='Survived', ascending=False), annot=True, cmap='BrBG')\n\nheatmap.set_title('Features Correlating with y', fontdict={'fontsize':18}, pad=16)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T20:34:56.96268Z","iopub.execute_input":"2021-12-09T20:34:56.962948Z","iopub.status.idle":"2021-12-09T20:34:57.282865Z","shell.execute_reply.started":"2021-12-09T20:34:56.962919Z","shell.execute_reply":"2021-12-09T20:34:57.281835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With this visualization, we can see that some traits have a very strong effect on the `Survival`","metadata":{}},{"cell_type":"code","source":"for i in ['Pclass','Age','SibSp']:\n    print(str(i)+' распределение признака на данных')\n    px.box(y=df[i].astype('float'), x=df['Survived']).show()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T20:37:11.13476Z","iopub.execute_input":"2021-12-09T20:37:11.135203Z","iopub.status.idle":"2021-12-09T20:37:12.502205Z","shell.execute_reply.started":"2021-12-09T20:37:11.135165Z","shell.execute_reply":"2021-12-09T20:37:12.501346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So okay, I didn't understand anything, it's very interesting, it will be interesting to fuck clustering instead of the target itself","metadata":{}},{"cell_type":"code","source":"df.Survived.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T20:40:03.013667Z","iopub.execute_input":"2021-12-09T20:40:03.014781Z","iopub.status.idle":"2021-12-09T20:40:03.023008Z","shell.execute_reply.started":"2021-12-09T20:40:03.014735Z","shell.execute_reply":"2021-12-09T20:40:03.022321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop('Survived', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T20:40:24.650275Z","iopub.execute_input":"2021-12-09T20:40:24.650542Z","iopub.status.idle":"2021-12-09T20:40:24.65635Z","shell.execute_reply.started":"2021-12-09T20:40:24.650515Z","shell.execute_reply":"2021-12-09T20:40:24.655414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using metrics\nIn this situation, when teaching unsupervised, it is better to use the `silhouette_score` metric. It is a method of interpreting and checking for consistency across data clusters. The technique provides a concise graphical representation of how well each object has been classified. The silhouette value is a measure of how similar an object is to its cluster compared to other clusters.","metadata":{}},{"cell_type":"code","source":"from sklearn import metrics","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:16:14.170694Z","iopub.execute_input":"2021-12-09T21:16:14.171258Z","iopub.status.idle":"2021-12-09T21:16:15.442744Z","shell.execute_reply.started":"2021-12-09T21:16:14.17103Z","shell.execute_reply":"2021-12-09T21:16:15.441837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop('isWeekday', axis=1, inplace=True)\ndf['date'] = pd.to_datetime(df['date'])\ndf['month'] = df['date'].apply(lambda x: x.month)\ndf['hour'] = df['date'].apply(lambda x: x.hour)\ndf['day'] = df['date'].apply(lambda x: x.day)\ndf['day_of_the_week'] = df['date'].apply(lambda x: x.isoweekday())\n\n\nimport seaborn as sns\ncorre = df.iloc[:, 3:].corr()\nsns.heatmap(corre, annot=True)\n\n\n\n-----\n\n\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\ntfidf = TfidfVectorizer(ngram_range=(1,3))\ntfidf_features = tfidf.fit_transform(df.text)\ntfidf_feature_names = tfidf.get_feature_names()\n \n\ncv = CountVectorizer(ngram_range=(1,3))\ncv_features = cv.fit_transform(df.text)\ncv_feature_names = cv.get_feature_names()\n\n----\n\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.cluster import KMeans\nn_clusters = [12,13,14,15,16,17,18,19,20] # number of clusters\nclusters_inertia = [] # inertia of clusters\ns_scores = [] # silhouette scores\n\nfor n in n_clusters:\n    KM_est = KMeans(n_clusters=n, init='k-means++').fit(features)\n    clusters_inertia.append(KM_est.inertia_)    # data for the elbow method\n    silhouette_avg = silhouette_score(features, KM_est.labels_)\n    s_scores.append(silhouette_avg) # data for the silhouette score method\n    \n----\n\n\nfig, ax = plt.subplots(figsize=(12,5))\nax = sns.lineplot(n_clusters, s_scores, marker='o', ax=ax)\nax.set_title(\"Silhouette score method\")\nax.set_xlabel(\"number of clusters\")\nax.set_ylabel(\"Silhouette score\")\nax.axvline(17, ls=\"--\", c=\"red\")\nplt.grid()\nplt.show()\n---\n\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\nimport gensim\nfrom gensim import corpora\nimport pyLDAvis.gensim\n\n\nno_topics = 15\n \n#NMF\nnmf_tfidf = NMF(n_components=10, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf_features)\n\n\n#LDA\nlda_cv = LatentDirichletAllocation(n_components=10, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(cv_features)\n\n\n---\n\ndef display_topics(model, feature_names, no_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n                 print (\"\\ntopic {}: {}\". format (topic_idx, \"|\" .join ([feature_names [i] for i in topic.argsort () [: - no_top_words-1: -1]])))\n \nno_top_words = 10\nprint ('--------------- Тема NMF-tfidf_features ---------------------------- ------------- ')\ndisplay_topics(nmf_tfidf, tfidf_feature_names, no_top_words)\nprint()\nprint ('-------------- Тема Lda-CountVectorizer_features ----------------------------- --- ')\ndisplay_topics(lda_cv, cv_feature_names, no_top_words)\n\n---\n#genism text clust\n\ntext_data = df.text.apply(lambda x:x.split())\n # Отфильтруйте слова одного китайского иероглифа\ntext_data = text_data.apply(lambda x:[w for w in x if len(w)>1] )\n \ndictionary = corpora.Dictionary(text_data)\n \n # Отфильтровать слова с частотой менее 5 раз и слова с частотой более 90%\ndictionary.filter_extremes(no_below=5, no_above=0.9)\n\n----\n\n# Счетный корпус\ncorpus = [dictionary.doc2bow(text) for text in text_data]\n \n # Обычная модель LDA\nimport gensim\nno_topics = 9\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = no_topics, id2word=dictionary)\n \ntopics = ldamodel.print_topics(num_words=8)\nfor topic in topics:\n    print (\"тема% d:\"% (topic[0]))\n    print(topic[1])\n    print()\n    \n    \n    \n----\n\n#Ниже мы используем корпус на основе TF-IDF для моделирования тем. Мы также указываем 7 тем. Здесь мы используем параллельную многоядерную модель LDA (LdaMulticore). Если ваш процессор многоядерный, вы можете использовать этот метод Для обучения модели это может сократить время обучения:\n\n# tf-idf корпус\ntfidf = gensim.models.TfidfModel(corpus)\ncorpus_tfidf = tfidf[corpus]\n \n # Многоядерная параллельная модель lda\nno_topics = 9\ntf_idf_lda_model = gensim.models.LdaMulticore(corpus_tfidf, num_topics=no_topics, id2word=dictionary, passes=2, workers=4)\n \ntopics = tf_idf_lda_model.print_topics(num_words=8)\nfor topic in topics:\n    print (\"тема% d:\"% (topic[0]))\n    print(topic[1])\n    print()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lda_display = pyLDAvis.gensim.prepare(tf_idf_lda_model, corpus_tfidf, dictionary, sort_topics=False)\npyLDAvis.display(lda_display)\nlda_display = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics=False)\npyLDAvis.display(lda_display)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Choosing a Clustering Model\nThere will be 3 clustering models under consideration:\n\nThe clustering model will be selected based on the result of `silhouette_score`. The higher the better.\nBut we will count this metric on 1/10 of the data, because it will take a lot of time to calculate the metric on all data.","metadata":{}},{"cell_type":"code","source":"X = df.drop([\"Survived\", 'Name', 'Sex','Ticket'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:19:45.345324Z","iopub.execute_input":"2021-12-09T21:19:45.345887Z","iopub.status.idle":"2021-12-09T21:19:45.351503Z","shell.execute_reply.started":"2021-12-09T21:19:45.34585Z","shell.execute_reply":"2021-12-09T21:19:45.350844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.drop('Embarked', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:20:55.80671Z","iopub.execute_input":"2021-12-09T21:20:55.807531Z","iopub.status.idle":"2021-12-09T21:20:55.81358Z","shell.execute_reply.started":"2021-12-09T21:20:55.807493Z","shell.execute_reply":"2021-12-09T21:20:55.81269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:20:31.282926Z","iopub.execute_input":"2021-12-09T21:20:31.283225Z","iopub.status.idle":"2021-12-09T21:20:31.293479Z","shell.execute_reply.started":"2021-12-09T21:20:31.283196Z","shell.execute_reply":"2021-12-09T21:20:31.292147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:20:58.058704Z","iopub.execute_input":"2021-12-09T21:20:58.059278Z","iopub.status.idle":"2021-12-09T21:20:58.076757Z","shell.execute_reply.started":"2021-12-09T21:20:58.05924Z","shell.execute_reply":"2021-12-09T21:20:58.07587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### k-means method\nThe most popular data clustering algorithm is the k-means method. This is an iterative clustering algorithm based on minimizing the total square deviations of cluster points from the centroids (mean coordinates) of these clusters","metadata":{}},{"cell_type":"code","source":"### k-means method\nfrom sklearn.cluster import KMeans\n\n# initialize the model\nmodel_kmeans = KMeans(n_clusters=2,random_state=0)\n# train our model\nmodel_kmeans.fit(X)\n# make predictions\npred_kmeans_test=model_kmeans.predict(X)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:21:01.125599Z","iopub.execute_input":"2021-12-09T21:21:01.12589Z","iopub.status.idle":"2021-12-09T21:21:01.182798Z","shell.execute_reply.started":"2021-12-09T21:21:01.125859Z","shell.execute_reply":"2021-12-09T21:21:01.182144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### EM algorithm\n  EM is a type of clustering algorithm. As the name suggests, each cluster is modeled according to a different Gaussian distribution. This flexible and probabilistic approach to data modeling means that instead of hard assignments to clusters like k-means, we have soft assignments. This means that each data point could have been generated by any of the distributions with the appropriate probability.","metadata":{}},{"cell_type":"code","source":"# EM algorithm\nfrom sklearn.mixture import GaussianMixture\n\n# initialize the model \nmodel_gaus = GaussianMixture(n_components=2)\n# train our model\nmodel_gaus.fit(X)\n# make predictions\npred_model_gaus_test=model_gaus.predict(X)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:21:36.697453Z","iopub.execute_input":"2021-12-09T21:21:36.697743Z","iopub.status.idle":"2021-12-09T21:21:36.755342Z","shell.execute_reply.started":"2021-12-09T21:21:36.697714Z","shell.execute_reply":"2021-12-09T21:21:36.754306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Birch clustering\nIt is an unsupervised data mining algorithm used to implement hierarchical clustering on large datasets. The advantage of `BIRCH` is the ability of the method to dynamically cluster as it receives multidimensional metric data points in an attempt to obtain better clustering for the available set of resources.","metadata":{}},{"cell_type":"code","source":"# Birch clustering\nfrom sklearn.cluster import Birch\n\n# initialize the model \nmodel_db = Birch(n_clusters=2)\n# train our model\nmodel_db.fit(X)\n# make predictions\npred_model_db_test=model_db.predict(X)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:22:03.076428Z","iopub.execute_input":"2021-12-09T21:22:03.076746Z","iopub.status.idle":"2021-12-09T21:22:03.216026Z","shell.execute_reply.started":"2021-12-09T21:22:03.076713Z","shell.execute_reply":"2021-12-09T21:22:03.214658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# See the result of the metric assessment\nmetrics.silhouette_score(X,pred_kmeans_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:22:15.946771Z","iopub.execute_input":"2021-12-09T21:22:15.947138Z","iopub.status.idle":"2021-12-09T21:22:15.981479Z","shell.execute_reply.started":"2021-12-09T21:22:15.947101Z","shell.execute_reply":"2021-12-09T21:22:15.980382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# See the result of the metric assessment\nmetrics.silhouette_score(X, pred_model_gaus_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:22:24.673877Z","iopub.execute_input":"2021-12-09T21:22:24.674169Z","iopub.status.idle":"2021-12-09T21:22:24.704806Z","shell.execute_reply.started":"2021-12-09T21:22:24.674136Z","shell.execute_reply":"2021-12-09T21:22:24.703982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# See the result of the metric assessment\nmetrics.silhouette_score(X, pred_model_db_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:22:32.355704Z","iopub.execute_input":"2021-12-09T21:22:32.355991Z","iopub.status.idle":"2021-12-09T21:22:32.389899Z","shell.execute_reply.started":"2021-12-09T21:22:32.355959Z","shell.execute_reply":"2021-12-09T21:22:32.389064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X['y'] = pred_kmeans_test","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:22:58.777555Z","iopub.execute_input":"2021-12-09T21:22:58.777878Z","iopub.status.idle":"2021-12-09T21:22:58.785913Z","shell.execute_reply.started":"2021-12-09T21:22:58.777845Z","shell.execute_reply":"2021-12-09T21:22:58.785006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.y.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:23:11.288628Z","iopub.execute_input":"2021-12-09T21:23:11.28894Z","iopub.status.idle":"2021-12-09T21:23:11.29725Z","shell.execute_reply.started":"2021-12-09T21:23:11.288911Z","shell.execute_reply":"2021-12-09T21:23:11.296596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"proc = len(df)*0.05\nfor col in df.iloc[:,8:].drop('y', axis=1).columns:\n        if df[col].sum()< proc:\n            df.drop(col, axis=1, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dic = {2:'red', 1:'yellow', 0:'green'}\n\ndf['color'] = df.y.map(dic)\n\nfig = px.scatter_mapbox(df,\n                        lat='klm',\n                        lon='longisland',\n                        hover_data=['Harmony', 'count'],\n                        color_discrete_sequence=[df.color])\n\nfig.update_layout(mapbox_style='open-street-map')\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(2):\n    print('\\n')\n    print('Median age of group '+str(i)+':\\n'+str(np.mean(X[X['y']==i]['Age'])))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:23:59.052984Z","iopub.execute_input":"2021-12-09T21:23:59.054328Z","iopub.status.idle":"2021-12-09T21:23:59.064046Z","shell.execute_reply.started":"2021-12-09T21:23:59.054252Z","shell.execute_reply":"2021-12-09T21:23:59.063309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 12))\nheatmap = sns.heatmap(df.corr()[['Survived']].sort_values(by='Survived', ascending=False), annot=True, cmap='BrBG')\n\nheatmap.set_title('Features Correlating with y', fontdict={'fontsize':18}, pad=16)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T20:34:56.96268Z","iopub.execute_input":"2021-12-09T20:34:56.962948Z","iopub.status.idle":"2021-12-09T20:34:57.282865Z","shell.execute_reply.started":"2021-12-09T20:34:56.962919Z","shell.execute_reply":"2021-12-09T20:34:57.281835Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classification methods\nIn testing models, 3 classification methods will be adopted:\n### AdaBoost Algorithm\nGood generalizing ability. In real problems (not always, but often) it is possible to build compositions that are superior in quality to the basic algorithms. The generalizing ability can improve (in some problems) as the number of basic algorithms increases.\nEase of implementation.\nThe inherent boosting overhead is low. The composition building time is almost completely determined by the training time of the basic algorithms.\nAbility to identify objects that are noise emissions.\n### Decision tree algorithm\nA decision tree is a useful technique for dividing a complex problem into smaller, more manageable subtasks. The solution of the problem using a decision tree is carried out in two stages. The first stage includes building a decision tree indicating all possible outcomes (financial results) and their probabilities.\n### Gradient Boosting Classifier\nRadiant boosting is an ensemble of decision trees. This algorithm is based on iterative training of decision trees in order to minimize the loss function. Due to the peculiarities of decision trees, gradient boosting is able to work with categorical features, cope with nonlinearities","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:26:50.794648Z","iopub.execute_input":"2021-12-09T21:26:50.794985Z","iopub.status.idle":"2021-12-09T21:26:50.80457Z","shell.execute_reply.started":"2021-12-09T21:26:50.79495Z","shell.execute_reply":"2021-12-09T21:26:50.803781Z"}}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.ensemble import GradientBoostingClassifier","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:27:05.032492Z","iopub.execute_input":"2021-12-09T21:27:05.033321Z","iopub.status.idle":"2021-12-09T21:27:05.073226Z","shell.execute_reply.started":"2021-12-09T21:27:05.033254Z","shell.execute_reply":"2021-12-09T21:27:05.072484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#импортируем библиотеку для разбиения\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:28:53.40944Z","iopub.execute_input":"2021-12-09T21:28:53.410088Z","iopub.status.idle":"2021-12-09T21:28:53.415194Z","shell.execute_reply.started":"2021-12-09T21:28:53.410037Z","shell.execute_reply":"2021-12-09T21:28:53.414164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"training and calculating the classification score (namely, calling `score`, which will give us an idea of the percentage of correctly guessed classes)\nAnd choose a model based on a quality assessment metric, the best model\n","metadata":{}},{"cell_type":"code","source":"X_test_train, X_test_test, y_test_train, y_test_test = train_test_split(X.drop('y', axis=1), X.y)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:28:55.767377Z","iopub.execute_input":"2021-12-09T21:28:55.767678Z","iopub.status.idle":"2021-12-09T21:28:55.776212Z","shell.execute_reply.started":"2021-12-09T21:28:55.767649Z","shell.execute_reply":"2021-12-09T21:28:55.775565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decision tree algorithm\ntree = DecisionTreeClassifier (random_state = 0, max_depth = 100)\n# train the model\ntree.fit (X_test_train, y_test_train)\n\n# AdaBoost algorithm\nada = AdaBoostClassifier ()\n# train the model\nada.fit (X_test_train, y_test_train)\n\n#Gradient Boosting Classifier\ngb = GradientBoostingClassifier ()\n# train the model\ngb.fit (X_test_train, y_test_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:29:09.226155Z","iopub.execute_input":"2021-12-09T21:29:09.22683Z","iopub.status.idle":"2021-12-09T21:29:09.327432Z","shell.execute_reply.started":"2021-12-09T21:29:09.226781Z","shell.execute_reply":"2021-12-09T21:29:09.326731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking for correctly predicted patterns\nThe `model.score ()` method compares the answers given by the classifier and the correct answers","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:29:17.641026Z","iopub.execute_input":"2021-12-09T21:29:17.641559Z","iopub.status.idle":"2021-12-09T21:29:17.647959Z","shell.execute_reply.started":"2021-12-09T21:29:17.641511Z","shell.execute_reply":"2021-12-09T21:29:17.64659Z"}}},{"cell_type":"code","source":"tree.score(X_test_test,y_test_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:29:26.589012Z","iopub.execute_input":"2021-12-09T21:29:26.589737Z","iopub.status.idle":"2021-12-09T21:29:26.598646Z","shell.execute_reply.started":"2021-12-09T21:29:26.589692Z","shell.execute_reply":"2021-12-09T21:29:26.59783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ada.score(X_test_test,y_test_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:29:34.508569Z","iopub.execute_input":"2021-12-09T21:29:34.509465Z","iopub.status.idle":"2021-12-09T21:29:34.5194Z","shell.execute_reply.started":"2021-12-09T21:29:34.509425Z","shell.execute_reply":"2021-12-09T21:29:34.518228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gb.score(X_test_test,y_test_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:29:39.386229Z","iopub.execute_input":"2021-12-09T21:29:39.386748Z","iopub.status.idle":"2021-12-09T21:29:39.399895Z","shell.execute_reply.started":"2021-12-09T21:29:39.386713Z","shell.execute_reply":"2021-12-09T21:29:39.398539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What I wanted to say with this work is that actually machine learning is easy to start and hard to beat. This is more of a creative work that requires a lot of work, it takes imagination to come up with not only the best model, but also the best solution for it.","metadata":{}},{"cell_type":"markdown","source":"![](http://quotefancy.com/media/wallpaper/3840x2160/20391-Nolan-Bushnell-Quote-A-good-game-is-easy-to-learn-but-hard-to.jpg)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import geocoder\nfrom datetime import datetime\nfrom sklearn import *\n\na = 'Your answer:'\n\n\ndef str2bool (v):\n    v = v.lower ()\n    if any (word in v for word in ['yes', \"aha\", \"of course\", \"always\"]):\n        return True\n    if any (word in v for word in ['no', \"nope\", \"never\"]):\n        return True\n    return\n\n\ndef take_inf ():\n    b_a ('I'll ask you to answer a couple of questions about the incident')\n    b_a ('Give the address of the country with the indication of the city')\n    g = geocoder.osm (input (a))\n    if g.ok == True:\n        region = g.region\n        lat = g.osm ['x']\n        lon = g.osm ['y']\n    else:\n        b_a ('Error in geocoding, will have to be written by hand')\n        b_a ('Write the name of the region in the imperative')\n        region = input (a)\n        b_a ('Write the longitude of the place where the accident occurred')\n        lon = input (a)\n        b_a ('Write the latitude of the place where the accident occurred')\n        lat = input (a)\n\n    b_a ('Is the date of the incident today?')\n    answer = input (a)\n    if str2bool (answer):\n        date = datetime.today ()\n    else:\n        b_a ('Write the date in this format with hours and minutes, example: 08/18/21 18:00'\n            )\n        date = datetime.striptime (input (a),\n                                  '% d.% m.% y. % H:% M '). Astype (' datetime64 ')\n    b_a ('Please indicate the number of survivors in this accident')\n    inj = (input (a))\n    b_a ('Enter the number of deaths')\n    death = int (input (a))\n    b_a ('In your opinion, how many accidents were there in this area')\n    count = int (input (a))\n    b_a ('What are the most common accidents in this area? (Severe, Medium, Light)'\n        )\n    severity = input (a)\n    if any (word in severity for word in (\"light\", \"small\", \"little\")):\n        severity = 2 * count\n    elif any (word in severity for word in (\"average\")):\n        severity = 2 * count\n    elif any (word in severity for word in (\"heavy\", \"death\", \"dangerous\")):\n        severity = 2 * count\n    b_a ('Does this section have problems with the road surface in winter?')\n    BadWinter = str2bool (input (a))\n    b_a ('Didn't this area have problems with poor lighting?')\n    BadVision = str2bool (input (a))\n    b_a ('Does this section have problems with misapplication of signs or other road traffic control tools?'\n        )\n    BadSign = str2bool (input (a))\n\n    b_a ('Didn't this section have any problems with the road surface?')\n    bad_road = str2bool (input ('bad_road'))\n    df = pd.DataFrame ([[\n        lat, lon, region, date, g.location, count, inj, death, severity,\n        BadSign, BadWinter, BadVision, bad_road\n    ]],\n                      columns = [\n                          'lat', 'lon', 'region', 'date', 'address', 'count',\n                          'inj_count', 'death_count', 'severity', \"BadWinter\",\n                          'BadVision', 'BadSign', 'BadRoad'\n                      ])\n    return df\n\n\ndef readcsv_df (file, encodding = None, sep = None, csv = False):\n    \"\" \"Function for reading data. Accepts both pd.core.DataFrame and csv file. (Parameter csv = True)\n    With the indication of the encoding and separator. Returns a Dataframe conditionally checked\n    That the columns match ['lat', 'lon', 'region', 'date', 'address',' inj_count ',' death_count ',' road_conditions', 'BadSign', \"BadWinter\", 'BadLight', 'bad_road '] \"\" \"\n    if (csv):\n        df = pd.read_csv (filefile, encodding = encodding, sep = sep)\n    else:\n        df = file\n    if set (df.columns)! = set ([\n            'lat', 'lon', 'region', 'date', 'address', 'count',\n                          'inj_count', 'death_count', 'severity', \"BadWinter\",\n                          'BadVision', 'BadSign', 'BadRoad'\n    ]):\n        print (\n            'Traceback: Reformat the file, make sure the parameters are correct'\n        )\n    else:\n        return df\n\n\ndef pred (df):\n    '' 'function for predicting the level of threat' ''\n    df = df.iloc [:, 5:]\n    model = pickle.load (open ('model.sav', 'rb'))\n    pred = model.predict (df)\n    if pred == 0:\n        res = 'Light site threat'\n    if pred == 1:\n        res = 'Medium Site Threat'\n    if pred == 2:\n        res = 'Severe threat level of the site'\n    return res\n\n\ndef count_y (gorod, y_level):\n    '' 'a function to count the number of sites with a certain level of threat in a certain city' ''\n    df = pd.read_csv ('result_with_y.csv')\n    return df [df ['y'] == y_level] .groupby (['region']). count (). loc [gorod] ['y']\n\n\ndef b_a (text):\n    '' 'function to avoid writing BOT every time:' ''\n    print ('Bot:' + str (text) + '\\ n')\n\n\ndef bot ():\n    n = 1\n    while (True):\n        if (n == 1):\n            b_a (\"My commands: \\ n1.Find out the number of hazardous areas in a certain region \\ n2. find out the hazard level of the area by its parameters \\ n3. Collecting data on hazardous areas \\ n To exit write complete\"\n                )\n        b_a (\"What do you want?\")\n        answer = input (a)\n\n        if any (word in answer for word in (\"how much\", \"I will pass\")):\n            b_a ('Error in the name of the city, please write the city in the nominativecase '\n                )\n            b_a ('Which areas are you interested in by the degree of danger?')\n            answer = input (a)\n            if any (word in answer.lower ()\n                   for word in (\"light\", \"small\", \"little\")):\n                y_level = 0\n            if any (word in answer.lower () for word in (\"average\")):\n                y_level = 1\n            if any (word in answer.lower ()\n                   for word in (\"heavy\", \"death\", \"dangerous\")):\n                y_level = 2\n            b_a ('Error in the name of the city, please write the city in the nominative case')\n            gorod = input (a)\n            b_a (count_y (gorod, y_level))\n            n = n + 1\n        if any (word in answer.lower ()\n               for word in ('find out', \"what\", \"level\")):\n            pred (readcsv_df (take_inf ()))\n            n = n + 1\n        if any (word in answer.lower ()\n               for word in (\"exit\", \"bye\", 'finish')):\n            b_a (\"Bye!\")\n            return 0\n        if any (word in answer.lower ()\n               for word in (\"help\", \"info\", 'commands')):\n            n = 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import geocoder\nfrom datetime import datetime\nfrom sklearn import *\n\n\ndef b (text):\n    print ('Bot:' + str (text))\n\n\na = 'You:'\n\n\ndef str2bool (v):\n    v = v.lower ()\n    if any (word in v for word in ['aha', \"yes\", \"of course\"]):\n        return True\n    if any (word in v for word in ['no', \"nope\", \"never\", \"never was\"]):\n        return False\n    b ('I do not understand you, please repeat')\n    str2bool (v)\n\n\ndef take_inf (orig = False):\n    if orig == False:\n        g = geocoder.osm (input (a))\n        if g.ok == True:\n            region = g.region\n            lat = g.osm ['x']\n            lon = g.osm ['y']\n        else:\n            region = input (a)\n            lat = input (a)\n            lon = input (a)\n\n        answer = input (a)\n        if str2boolbool (answer):\n            date = datetime.today ()\n        else:\n            date = datetime.striptime (input (a),\n                                      '% d.% m.%. y. % H:% M '). Astype (' datetime64 ')\n\n    inj = int (input (a))\n    dead = int (input (a))\n    count = int (input (a))\n    severity = input (a) .lower ()\n    if any (word in severity for word in ['leg', \"small\"]):\n        severity = count\n    elif any (word in severity for word in [\n            'average',\n            \"normal\",\n    ]):\n        severity = count * 2\n    elif any (word in severity\n           for word in ['heavy', \"death\", \"dangerous\"]):\n        severity = count * 3\n    else:\n        b ('Dont_understand')\n            severity = input (a) .lower ()\n        if any (word in severity for word in ['leg', \"small\"]):\n            severity = count\n        elif any (word in severity for word in [\n                'average',\n                \"normal\",\n        ]):\n            severity = count * 2\n        elif any (word in severity\n               for word in ['heavy', \"death\", \"dangerous\"]):\n            severity = count * 3\n    Wet = str2bool (input (a))\n    BadWinter = str2bool (input (a))\n    BadSign = str2bool (input (a))\n    BadVision = str2bool (input (a))\n\n    if orig == False:\n        df = pd.DataFrame ([[\n            lat, lon, region, date, g.location, count, inj, death, severity,\n            Wet, BadWinter, BadSign, BadVision\n        ]],\n                          columns = [\n                              'lat', 'lon', 'region', 'date', 'address',\n                              'count', 'inj', 'death', 'severity', 'Wet',\n                              'BadWinter', 'BadSign', 'BadVision'\n                          ])\n    else:\n        df = pd.DataFrame (\n            [[count, inj, dead, severity, Wet, BadWinter, BadSign, BadVision]\n             ],\n            columns = [\n                'count', 'inj', 'death', 'severity', 'Wet', 'BadWinter',\n                'BadSign', 'BadVision'\n            ])\n    return df","metadata":{},"execution_count":null,"outputs":[]}]}
